{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ba740-4b44-4e03-8bfc-205736f7d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM:\n",
      "    Accuracy: 0.6840\n",
      "    Precision: 0.6568\n",
      "    Recall: 0.7400\n",
      "    F1 Score: 0.6959\n",
      "\n",
      "Results for Random Forest:\n",
      "    Accuracy: 0.7785\n",
      "    Precision: 0.7562\n",
      "    Recall: 0.8067\n",
      "    F1 Score: 0.7806\n",
      "\n",
      "Results for Logistic Regression:\n",
      "    Accuracy: 0.7231\n",
      "    Precision: 0.7044\n",
      "    Recall: 0.7467\n",
      "    F1 Score: 0.7249\n",
      "\n",
      "Results for Naive Bayes:\n",
      "    Accuracy: 0.6743\n",
      "    Precision: 0.6667\n",
      "    Recall: 0.6667\n",
      "    F1 Score: 0.6667\n",
      "\n",
      "Results for K-NN:\n",
      "    Accuracy: 0.6319\n",
      "    Precision: 0.6022\n",
      "    Recall: 0.7267\n",
      "    F1 Score: 0.6586\n",
      "\n",
      "Results for Gradient Boosting:\n",
      "    Accuracy: 0.7785\n",
      "    Precision: 0.7470\n",
      "    Recall: 0.8267\n",
      "    F1 Score: 0.7848\n",
      "\n",
      "Results for AdaBoost:\n",
      "    Accuracy: 0.7622\n",
      "    Precision: 0.7516\n",
      "    Recall: 0.7667\n",
      "    F1 Score: 0.7591\n",
      "\n",
      "Results for Decision Tree:\n",
      "    Accuracy: 0.6319\n",
      "    Precision: 0.6242\n",
      "    Recall: 0.6200\n",
      "    F1 Score: 0.6221\n",
      "\n",
      "Results for Perceptron:\n",
      "    Accuracy: 0.6417\n",
      "    Precision: 0.6220\n",
      "    Recall: 0.6800\n",
      "    F1 Score: 0.6497\n",
      "\n",
      "Results for SGD:\n",
      "    Accuracy: 0.5765\n",
      "    Precision: 0.5370\n",
      "    Recall: 0.9667\n",
      "    F1 Score: 0.6905\n",
      "\n",
      "Results for Bagging:\n",
      "    Accuracy: 0.7915\n",
      "    Precision: 0.7688\n",
      "    Recall: 0.8200\n",
      "    F1 Score: 0.7935\n",
      "\n",
      "Results for Extra Trees:\n",
      "    Accuracy: 0.7687\n",
      "    Precision: 0.7365\n",
      "    Recall: 0.8200\n",
      "    F1 Score: 0.7760\n",
      "\n",
      "Results for CatBoost:\n",
      "    Accuracy: 0.7850\n",
      "    Precision: 0.7500\n",
      "    Recall: 0.8400\n",
      "    F1 Score: 0.7925\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6375\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Results for LightGBM:\n",
      "    Accuracy: 0.7687\n",
      "    Precision: 0.7453\n",
      "    Recall: 0.8000\n",
      "    F1 Score: 0.7717\n",
      "\n",
      "Results for XGBoost:\n",
      "    Accuracy: 0.8013\n",
      "    Precision: 0.7871\n",
      "    Recall: 0.8133\n",
      "    F1 Score: 0.8000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluation and Voting Classifier setup (using hard voting to avoid the AttributeError)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m VotingClassifier(estimators\u001b[38;5;241m=\u001b[39m[(name, clf) \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems()], voting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhard\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m voting_clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     88\u001b[0m y_pred_vote \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoting Classifier Performance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:346\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    344\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, transformed_y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     )\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m     82\u001b[0m     delayed(_fit_single_estimator)(\n\u001b[0;32m     83\u001b[0m         clone(clf),\n\u001b[0;32m     84\u001b[0m         X,\n\u001b[0;32m     85\u001b[0m         y,\n\u001b[0;32m     86\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m     87\u001b[0m         message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVoting\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(names[idx], idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(clfs)),\n\u001b[0;32m     89\u001b[0m     )\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clfs)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clf \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m )\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:46\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[1;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m---> 46\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    539\u001b[0m     X,\n\u001b[0;32m    540\u001b[0m     y,\n\u001b[0;32m    541\u001b[0m     raw_predictions,\n\u001b[0;32m    542\u001b[0m     sample_weight,\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    544\u001b[0m     X_val,\n\u001b[0;32m    545\u001b[0m     y_val,\n\u001b[0;32m    546\u001b[0m     sample_weight_val,\n\u001b[0;32m    547\u001b[0m     begin_at_stage,\n\u001b[0;32m    548\u001b[0m     monitor,\n\u001b[0;32m    549\u001b[0m )\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    608\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    609\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    610\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    611\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    612\u001b[0m     )\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 615\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    616\u001b[0m     i,\n\u001b[0;32m    617\u001b[0m     X,\n\u001b[0;32m    618\u001b[0m     y,\n\u001b[0;32m    619\u001b[0m     raw_predictions,\n\u001b[0;32m    620\u001b[0m     sample_weight,\n\u001b[0;32m    621\u001b[0m     sample_mask,\n\u001b[0;32m    622\u001b[0m     random_state,\n\u001b[0;32m    623\u001b[0m     X_csc,\n\u001b[0;32m    624\u001b[0m     X_csr,\n\u001b[0;32m    625\u001b[0m )\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    254\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    256\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 257\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    260\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    261\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    262\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Function to transform sequence data based on the encoding scheme\n",
    "def transform_sequence(sequence, encoding):\n",
    "    numerical_data = []\n",
    "    for i in range(0, len(sequence), 2):  # Assuming the sequences can be divided into di-nucleotides\n",
    "        di_nucleotide = sequence[i:i+2]\n",
    "        numerical_data.append(encoding.get(di_nucleotide, 0))  # Default 0 for unknown di-nucleotides\n",
    "    \n",
    "    # Split the numerical data into non-overlapping windows of size 10 and calculate the average\n",
    "    window_avgs = [np.mean(numerical_data[i:i+10]) for i in range(0, len(numerical_data), 10)]\n",
    "    return window_avgs\n",
    "\n",
    "# Encoding scheme\n",
    "encoding = {\n",
    "    'AA': 0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -20, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': 0\n",
    "}\n",
    "\n",
    "# Function to load and transform data\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    # Handle varying window counts by padding with NaN and later imputing or dropping\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "# Load and transform data\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    # Excluded LDA and QDA due to collinearity warning\n",
    "    'SVM': SVC(probability=True, random_state=101),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=101),\n",
    "    'Logistic Regression': LogisticRegression(random_state=101),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=101),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=101),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=101),\n",
    "    'Perceptron': Perceptron(random_state=101),\n",
    "    'SGD': SGDClassifier(random_state=101),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=101),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=101),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=101),\n",
    "    'LightGBM': LGBMClassifier(random_state=101),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"    Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"    Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\\n\")\n",
    "\n",
    "# Evaluation and Voting Classifier setup (using hard voting to avoid the AttributeError)\n",
    "voting_clf = VotingClassifier(estimators=[(name, clf) for name, clf in classifiers.items()], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_vote = voting_clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Voting Classifier Performance:\")\n",
    "print(f\"    Accuracy: {accuracy_score(y_test, y_pred_vote):.4f}\")\n",
    "print(f\"    Precision: {precision_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n",
    "print(f\"    Recall: {recall_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n",
    "print(f\"    F1 Score: {f1_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6ae313-3e31-4ee2-9ab3-1ec359ec8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'classification_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def transform_sequence(sequence, encoding, window_size=None):\n",
    "    # Convert sequence to numerical data based on encoding\n",
    "    numerical_data = [encoding.get(sequence[i:i+2], 0) for i in range(0, len(sequence), 2)]\n",
    "    \n",
    "    # If window_size is None or greater than sequence length, return the mean of the entire sequence\n",
    "    if window_size is None or window_size >= len(numerical_data):\n",
    "        return [np.mean(numerical_data)]\n",
    "    \n",
    "    # Otherwise, calculate window averages\n",
    "    return [np.mean(numerical_data[i:i+window_size]) for i in range(0, len(numerical_data), window_size)]\n",
    "\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding, window_size=None):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding, window_size) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    \n",
    "    # Ensure uniform feature shape by padding with NaN and later filling with 0\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "# Encoding scheme\n",
    "encoding = {\n",
    "    'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "}\n",
    "\n",
    "window_sizes = [None, 5, 10, 15, 20]  # Define desired window sizes including 'None' for no window\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Use a simple classifier for demonstration\n",
    "    clf = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Store results\n",
    "    window_desc = 'No window' if window_size is None else f'Window size {window_size}'\n",
    "    results[window_desc] = [\n",
    "        accuracy_score(y_test, y_pred),\n",
    "        precision_score(y_test, y_pred, zero_division=0),\n",
    "        recall_score(y_test, y_pred, zero_division=0),\n",
    "        f1_score(y_test, y_pred, zero_division=0)\n",
    "    ]\n",
    "\n",
    "results.index = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "results.to_excel('classification_results.xlsx')\n",
    "\n",
    "# Print out to confirm completion\n",
    "print(\"Results saved to 'classification_results.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1382d0d9-203b-4b62-bcc3-173716d6ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 1237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 255\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 1\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495514 -> initscore=-0.017945\n",
      "[LightGBM] [Info] Start training from score -0.017945\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 1237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12646\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 50\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495514 -> initscore=-0.017945\n",
      "[LightGBM] [Info] Start training from score -0.017945\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 1237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6375\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495514 -> initscore=-0.017945\n",
      "[LightGBM] [Info] Start training from score -0.017945\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 1237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495514 -> initscore=-0.017945\n",
      "[LightGBM] [Info] Start training from score -0.017945\n",
      "[LightGBM] [Info] Number of positive: 1215, number of negative: 1237\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3315\n",
      "[LightGBM] [Info] Number of data points in the train set: 2452, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495514 -> initscore=-0.017945\n",
      "[LightGBM] [Info] Start training from score -0.017945\n",
      "Results saved to 'classification_results_with_windows.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             matthews_corrcoef, roc_auc_score, log_loss)\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
    "                              AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def transform_sequence(sequence, encoding, window_size=None):\n",
    "    numerical_data = [encoding.get(sequence[i:i+2], 0) for i in range(0, len(sequence), 2)]\n",
    "    if window_size is None or window_size >= len(numerical_data):\n",
    "        return [np.mean(numerical_data)]\n",
    "    return [np.mean(numerical_data[i:i+window_size]) for i in range(0, len(numerical_data), window_size)]\n",
    "\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding, window_size=None):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding, window_size) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "encoding = {\n",
    "    'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "}\n",
    "\n",
    "window_sizes = [None, 5, 10, 15, 20]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "classifiers = {\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'SGD': SGDClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        pipeline = make_pipeline(StandardScaler(), clf)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(clf, \"predict_proba\") else [0.5] * len(y_pred)\n",
    "        \n",
    "        window_desc = f\"{name} - {'No window' if window_size is None else f'Window {window_size}'}\"\n",
    "        results[window_desc] = [\n",
    "            accuracy_score(y_test, y_pred),\n",
    "            precision_score(y_test, y_pred, zero_division=0),\n",
    "            recall_score(y_test, y_pred, zero_division=0),\n",
    "            f1_score(y_test, y_pred, zero_division=0),\n",
    "            matthews_corrcoef(y_test, y_pred),\n",
    "            roc_auc_score(y_test, y_pred_proba) if hasattr(clf, \"predict_proba\") else np.nan,\n",
    "            log_loss(y_test, y_pred_proba) if hasattr(clf, \"predict_proba\") else np.nan\n",
    "        ]\n",
    "\n",
    "results.index = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC', 'ROC AUC', 'Log Loss']\n",
    "results.to_excel('classification_results_with_windows.xlsx')\n",
    "\n",
    "print(\"Results saved to 'classification_results_with_windows.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cd4691-8b96-4ed3-8f8d-864d31f81340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'classification_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def transform_sequence(sequence, encoding, window_size=None):\n",
    "    numerical_data = [encoding.get(sequence[i:i+2], 0) for i in range(0, len(sequence), 2)]\n",
    "    if window_size is None or window_size >= len(numerical_data):\n",
    "        return [np.mean(numerical_data)]\n",
    "    return [np.mean(numerical_data[i:i+window_size]) for i in range(0, len(numerical_data), window_size)]\n",
    "\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding, window_size=None):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding, window_size) for seq in sequences if seq]\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = label\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "encoding = {\n",
    "    'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "}\n",
    "\n",
    "\n",
    "window_sizes = [None, 5, 10, 15, 20]\n",
    "results = []\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding, window_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    window_desc = 'No window' if window_size is None else f'Window size {window_size}'\n",
    "    results.append({\n",
    "        'Window': window_desc,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Log Loss': log_loss(y_test, y_pred_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "\n",
    "print(\"Results saved to 'classification_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d6ffba-b7ec-46c0-b4df-e6605642e0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_14740\\86752975.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6217\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 125\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Results saved to 'classification_results_with_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             matthews_corrcoef, roc_auc_score)\n",
    "from sklearn.ensemble import (VotingClassifier, RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def transform_sequence(sequence, encoding):\n",
    "    numerical_data = [encoding.get(sequence[i:i+2], 0) for i in range(0, len(sequence), 2)]\n",
    "    window_size = 2  # Fixed window size\n",
    "    # Calculate the average for non-overlapping windows of size 15\n",
    "    window_avgs = [np.mean(numerical_data[i:i+window_size]) for i in range(0, len(numerical_data), window_size)]\n",
    "    return window_avgs\n",
    "\n",
    "encoding = {\n",
    "    'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "}\n",
    "\n",
    "\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "\n",
    "classifiers = {\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'SGD': SGDClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# At the beginning, ensure results is a DataFrame\n",
    "results = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'MCC', 'AUC'])\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics = {\n",
    "        'Classifier': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'MCC': matthews_corrcoef(y_test, y_pred)\n",
    "    }\n",
    "    # Calculate AUC for classifiers that have predict_proba\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "        metrics['AUC'] = roc_auc_score(y_test, y_pred_proba)\n",
    "    else:\n",
    "        metrics['AUC'] = 'N/A'\n",
    "    \n",
    "    # Temporarily convert metrics to DataFrame for appending\n",
    "    temp_df = pd.DataFrame([metrics])\n",
    "    results = pd.concat([results, temp_df], ignore_index=True)\n",
    "\n",
    "results.to_csv('classification_results_with_metrics.csv', index=False)\n",
    "print(\"Results saved to 'classification_results_with_metrics.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9a27e4-17b4-443a-8d1f-0fc18e350428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM\n",
      "Evaluating Random Forest\n",
      "Evaluating Gradient Boosting\n",
      "Evaluating AdaBoost\n",
      "Evaluating Decision Tree\n",
      "Evaluating Perceptron\n",
      "Evaluating SGD\n",
      "Evaluating Bagging\n",
      "Evaluating Extra Trees\n",
      "Evaluating CatBoost\n",
      "Evaluating LightGBM\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001212 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1379, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499819 -> initscore=-0.000725\n",
      "[LightGBM] [Info] Start training from score -0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1379\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500181 -> initscore=0.000725\n",
      "[LightGBM] [Info] Start training from score 0.000725\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000811 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 1380, number of negative: 1380\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 2760, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Evaluating XGBoost\n",
      "10-fold cross-validation results saved to 'classification_results_10_fold_cv.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Function to transform sequence data based on the encoding scheme\n",
    "def transform_sequence(sequence, encoding):\n",
    "    # Convert sequence to numerical data based on encoding\n",
    "    numerical_data = [encoding.get(sequence[i:i+2], 0) for i in range(0, len(sequence), 2)]\n",
    "    window_size = 15  # Fixed window size\n",
    "    # Calculate the average for non-overlapping windows of size 15\n",
    "    window_avgs = [np.mean(numerical_data[i:i+window_size]) for i in range(0, len(numerical_data), window_size)]\n",
    "    return window_avgs\n",
    "\n",
    "encoding = {\n",
    "    'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "    'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "    'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "    'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "}\n",
    "# Function to load and transform data\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    # Handle varying window counts by padding with NaN and later filling with 0\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "\n",
    "classifiers = {\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Perceptron': Perceptron(random_state=42),\n",
    "    'SGD': SGDClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "\n",
    "# Prepare for 10-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=101)\n",
    "scorers = {\n",
    "    'Accuracy': make_scorer(accuracy_score),\n",
    "    'Precision': make_scorer(precision_score, zero_division=0),\n",
    "    'Recall': make_scorer(recall_score),\n",
    "    'F1 Score': make_scorer(f1_score),\n",
    "    'ROC AUC': 'roc_auc'  # Directly use 'roc_auc' for AUC scoring\n",
    "}\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Evaluating {name}\")\n",
    "    scores = {metric: cross_val_score(clf, X, y, scoring=scorer, cv=cv) for metric, scorer in scorers.items()}\n",
    "    results[name] = [scores['Accuracy'].mean(), scores['Precision'].mean(), scores['Recall'].mean(), scores['F1 Score'].mean(), scores['ROC AUC'].mean()]\n",
    "\n",
    "results.index = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "results.to_csv('classification_results_10_fold_cv.csv')\n",
    "\n",
    "print(\"10-fold cross-validation results saved to 'classification_results_10_fold_cv.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63e9408f-8f27-4a61-8838-ca2866de0914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM:\n",
      "    Accuracy: 0.6645\n",
      "    Precision: 0.6343\n",
      "    Recall: 0.7400\n",
      "    F1 Score: 0.6831\n",
      "\n",
      "Results for Random Forest:\n",
      "    Accuracy: 0.6775\n",
      "    Precision: 0.6564\n",
      "    Recall: 0.7133\n",
      "    F1 Score: 0.6837\n",
      "\n",
      "Results for Gradient Boosting:\n",
      "    Accuracy: 0.6645\n",
      "    Precision: 0.6374\n",
      "    Recall: 0.7267\n",
      "    F1 Score: 0.6791\n",
      "\n",
      "Results for AdaBoost:\n",
      "    Accuracy: 0.6515\n",
      "    Precision: 0.6229\n",
      "    Recall: 0.7267\n",
      "    F1 Score: 0.6708\n",
      "\n",
      "Results for Decision Tree:\n",
      "    Accuracy: 0.6319\n",
      "    Precision: 0.6276\n",
      "    Recall: 0.6067\n",
      "    F1 Score: 0.6169\n",
      "\n",
      "Results for Perceptron:\n",
      "    Accuracy: 0.4886\n",
      "    Precision: 0.4886\n",
      "    Recall: 1.0000\n",
      "    F1 Score: 0.6565\n",
      "\n",
      "Results for SGD:\n",
      "    Accuracy: 0.6938\n",
      "    Precision: 0.6538\n",
      "    Recall: 0.7933\n",
      "    F1 Score: 0.7169\n",
      "\n",
      "Results for Bagging:\n",
      "    Accuracy: 0.6743\n",
      "    Precision: 0.6562\n",
      "    Recall: 0.7000\n",
      "    F1 Score: 0.6774\n",
      "\n",
      "Results for Extra Trees:\n",
      "    Accuracy: 0.6775\n",
      "    Precision: 0.6545\n",
      "    Recall: 0.7200\n",
      "    F1 Score: 0.6857\n",
      "\n",
      "Results for CatBoost:\n",
      "    Accuracy: 0.6840\n",
      "    Precision: 0.6568\n",
      "    Recall: 0.7400\n",
      "    F1 Score: 0.6959\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001765 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3817\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Results for LightGBM:\n",
      "    Accuracy: 0.6482\n",
      "    Precision: 0.6207\n",
      "    Recall: 0.7200\n",
      "    F1 Score: 0.6667\n",
      "\n",
      "Results for XGBoost:\n",
      "    Accuracy: 0.6352\n",
      "    Precision: 0.6188\n",
      "    Recall: 0.6600\n",
      "    F1 Score: 0.6387\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3817\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Voting Classifier Performance:\n",
      "    Accuracy: 0.6808\n",
      "    Precision: 0.6548\n",
      "    Recall: 0.7333\n",
      "    F1 Score: 0.6918\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b178ae5-7ce5-4a5e-8e7f-0fcba69361bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM:\n",
      "    Accuracy: 0.7647\n",
      "    Precision: 0.7857\n",
      "    Recall: 0.7857\n",
      "    F1 Score: 0.7857\n",
      "\n",
      "Results for Random Forest:\n",
      "    Accuracy: 0.7255\n",
      "    Precision: 0.7333\n",
      "    Recall: 0.7857\n",
      "    F1 Score: 0.7586\n",
      "\n",
      "Results for Logistic Regression:\n",
      "    Accuracy: 0.6863\n",
      "    Precision: 0.7143\n",
      "    Recall: 0.7143\n",
      "    F1 Score: 0.7143\n",
      "\n",
      "Results for Naive Bayes:\n",
      "    Accuracy: 0.7843\n",
      "    Precision: 0.7931\n",
      "    Recall: 0.8214\n",
      "    F1 Score: 0.8070\n",
      "\n",
      "Results for K-NN:\n",
      "    Accuracy: 0.6471\n",
      "    Precision: 0.7500\n",
      "    Recall: 0.5357\n",
      "    F1 Score: 0.6250\n",
      "\n",
      "Results for Gradient Boosting:\n",
      "    Accuracy: 0.6667\n",
      "    Precision: 0.7200\n",
      "    Recall: 0.6429\n",
      "    F1 Score: 0.6792\n",
      "\n",
      "Results for AdaBoost:\n",
      "    Accuracy: 0.6863\n",
      "    Precision: 0.6875\n",
      "    Recall: 0.7857\n",
      "    F1 Score: 0.7333\n",
      "\n",
      "Results for Decision Tree:\n",
      "    Accuracy: 0.5686\n",
      "    Precision: 0.6154\n",
      "    Recall: 0.5714\n",
      "    F1 Score: 0.5926\n",
      "\n",
      "Results for Perceptron:\n",
      "    Accuracy: 0.5882\n",
      "    Precision: 0.6522\n",
      "    Recall: 0.5357\n",
      "    F1 Score: 0.5882\n",
      "\n",
      "Results for SGD:\n",
      "    Accuracy: 0.6275\n",
      "    Precision: 0.6098\n",
      "    Recall: 0.8929\n",
      "    F1 Score: 0.7246\n",
      "\n",
      "Results for Bagging:\n",
      "    Accuracy: 0.7451\n",
      "    Precision: 0.8000\n",
      "    Recall: 0.7143\n",
      "    F1 Score: 0.7547\n",
      "\n",
      "Results for Extra Trees:\n",
      "    Accuracy: 0.7647\n",
      "    Precision: 0.8333\n",
      "    Recall: 0.7143\n",
      "    F1 Score: 0.7692\n",
      "\n",
      "Results for CatBoost:\n",
      "    Accuracy: 0.7647\n",
      "    Precision: 0.8077\n",
      "    Recall: 0.7500\n",
      "    F1 Score: 0.7778\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 230\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 455, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494505 -> initscore=-0.021979\n",
      "[LightGBM] [Info] Start training from score -0.021979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Results for LightGBM:\n",
      "    Accuracy: 0.7451\n",
      "    Precision: 0.7419\n",
      "    Recall: 0.8214\n",
      "    F1 Score: 0.7797\n",
      "\n",
      "Results for XGBoost:\n",
      "    Accuracy: 0.7451\n",
      "    Precision: 0.7586\n",
      "    Recall: 0.7857\n",
      "    F1 Score: 0.7719\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 230\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 455, number of used features: 75\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.494505 -> initscore=-0.021979\n",
      "[LightGBM] [Info] Start training from score -0.021979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Perceptron' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m voting_clf_soft\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Make predictions and evaluate\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m y_pred_soft \u001b[38;5;241m=\u001b[39m voting_clf_soft\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoft Voting Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_test,\u001b[38;5;250m \u001b[39my_pred_soft)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Train the hard voting classifier and evaluate (if needed)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:363\u001b[0m, in \u001b[0;36mVotingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    361\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoting \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 363\u001b[0m     maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# 'hard' voting\u001b[39;00m\n\u001b[0;32m    366\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:404\u001b[0m, in \u001b[0;36mVotingClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    Weighted average probability for each class per sample.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    403\u001b[0m avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_probas(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights_not_none\n\u001b[0;32m    405\u001b[0m )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:379\u001b[0m, in \u001b[0;36mVotingClassifier._collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:379\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_collect_probas\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([clf\u001b[38;5;241m.\u001b[39mpredict_proba(X) \u001b[38;5;28;01mfor\u001b[39;00m clf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Perceptron' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Function to transform sequence data based on the encoding scheme\n",
    "def transform_sequence(sequence, encoding):\n",
    "    numerical_data = []\n",
    "    for i in range(0, len(sequence), 2):  # Assuming the sequences can be divided into di-nucleotides\n",
    "        di_nucleotide = sequence[i:i+2]\n",
    "        numerical_data.append(encoding.get(di_nucleotide, 0))  # Default 0 for unknown di-nucleotides\n",
    "    \n",
    "    # Split the numerical data into non-overlapping windows of size 10 and calculate the average\n",
    "    window_avgs = [np.mean(numerical_data[i:i+2]) for i in range(0, len(numerical_data), 2)]\n",
    "    return window_avgs\n",
    "\n",
    "# Encoding scheme\n",
    "encoding = {\n",
    "    'AA': 0.00, 'AC': 0.50, 'AG': 0.50, 'AT': 0.00,\n",
    "    'CA': 0.50, 'CC': 1.00, 'CG': 1.00, 'CT': 0.50,\n",
    "    'GA': 0.50, 'GC': 1.00, 'GG': 1.00, 'GT': 0.50,\n",
    "    'TA': 0.00, 'TC': 0.50, 'TG': 0.50, 'TT': 0.00\n",
    "}\n",
    "\n",
    "# Function to load and transform data\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    # Handle varying window counts by padding with NaN and later imputing or dropping\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "# Load and transform data\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)\n",
    "\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    # Excluded LDA and QDA due to collinearity warning\n",
    "    'SVM': SVC(probability=True, random_state=101),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=101),\n",
    "    'Logistic Regression': LogisticRegression(random_state=101),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=101),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=101),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=101),\n",
    "    'Perceptron': Perceptron(random_state=101),\n",
    "    'SGD': SGDClassifier(random_state=101),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=101),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=101),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=101),\n",
    "    'LightGBM': LGBMClassifier(random_state=101),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)\n",
    "}\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"    Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"    Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\\n\")\n",
    "\n",
    "# Create a list of tuples containing classifier name and the classifier object\n",
    "classifier_list = [(name, clf) for name, clf in classifiers.items()]\n",
    "\n",
    "# Create a VotingClassifier with soft voting\n",
    "voting_clf_soft = VotingClassifier(estimators=classifier_list, voting='soft')\n",
    "\n",
    "# Optionally, create a VotingClassifier with hard voting\n",
    "voting_clf_hard = VotingClassifier(estimators=classifier_list, voting='hard')\n",
    "\n",
    "# Train the ensemble classifier\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "print(f\"Soft Voting Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "\n",
    "# Train the hard voting classifier and evaluate (if needed)\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "print(f\"Hard Voting Accuracy: {accuracy_score(y_test, y_pred_hard):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Voting Classifier Performance:\")\n",
    "print(f\"    Accuracy: {accuracy_score(y_test, y_pred_vote):.4f}\")\n",
    "print(f\"    Precision: {precision_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n",
    "print(f\"    Recall: {recall_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n",
    "print(f\"    F1 Score: {f1_score(y_test, y_pred_vote, zero_division=0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a148d5b-ddae-4c02-99d4-8f1e2005a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVM:\n",
      "    Accuracy: 0.7850\n",
      "    Precision: 0.7471\n",
      "    Recall: 0.8467\n",
      "    F1 Score: 0.7937\n",
      "\n",
      "Results for Random Forest:\n",
      "    Accuracy: 0.7915\n",
      "    Precision: 0.7622\n",
      "    Recall: 0.8333\n",
      "    F1 Score: 0.7962\n",
      "\n",
      "Results for Logistic Regression:\n",
      "    Accuracy: 0.7818\n",
      "    Precision: 0.7578\n",
      "    Recall: 0.8133\n",
      "    F1 Score: 0.7846\n",
      "\n",
      "Results for Naive Bayes:\n",
      "    Accuracy: 0.8046\n",
      "    Precision: 0.7711\n",
      "    Recall: 0.8533\n",
      "    F1 Score: 0.8101\n",
      "\n",
      "Results for K-NN:\n",
      "    Accuracy: 0.7296\n",
      "    Precision: 0.7006\n",
      "    Recall: 0.7800\n",
      "    F1 Score: 0.7382\n",
      "\n",
      "Results for Gradient Boosting:\n",
      "    Accuracy: 0.7687\n",
      "    Precision: 0.7484\n",
      "    Recall: 0.7933\n",
      "    F1 Score: 0.7702\n",
      "\n",
      "Results for AdaBoost:\n",
      "    Accuracy: 0.7785\n",
      "    Precision: 0.7562\n",
      "    Recall: 0.8067\n",
      "    F1 Score: 0.7806\n",
      "\n",
      "Results for Decision Tree:\n",
      "    Accuracy: 0.7003\n",
      "    Precision: 0.6747\n",
      "    Recall: 0.7467\n",
      "    F1 Score: 0.7089\n",
      "\n",
      "Results for Bagging:\n",
      "    Accuracy: 0.7883\n",
      "    Precision: 0.7707\n",
      "    Recall: 0.8067\n",
      "    F1 Score: 0.7883\n",
      "\n",
      "Results for Extra Trees:\n",
      "    Accuracy: 0.7883\n",
      "    Precision: 0.7673\n",
      "    Recall: 0.8133\n",
      "    F1 Score: 0.7896\n",
      "\n",
      "Results for CatBoost:\n",
      "    Accuracy: 0.7850\n",
      "    Precision: 0.7593\n",
      "    Recall: 0.8200\n",
      "    F1 Score: 0.7885\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 431\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Results for LightGBM:\n",
      "    Accuracy: 0.7850\n",
      "    Precision: 0.7561\n",
      "    Recall: 0.8267\n",
      "    F1 Score: 0.7898\n",
      "\n",
      "Results for XGBoost:\n",
      "    Accuracy: 0.7590\n",
      "    Precision: 0.7317\n",
      "    Recall: 0.8000\n",
      "    F1 Score: 0.7643\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 431\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Soft Voting Classifier Performance:\n",
      "    Accuracy: 0.7915\n",
      "    Precision: 0.7560\n",
      "    Recall: 0.8467\n",
      "    F1 Score: 0.7987\n",
      "[LightGBM] [Info] Number of positive: 1383, number of negative: 1376\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 431\n",
      "[LightGBM] [Info] Number of data points in the train set: 2759, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501269 -> initscore=0.005074\n",
      "[LightGBM] [Info] Start training from score 0.005074\n",
      "Hard Voting Classifier Performance:\n",
      "    Accuracy: 0.7818\n",
      "    Precision: 0.7515\n",
      "    Recall: 0.8267\n",
      "    F1 Score: 0.7873\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Function to transform sequence data based on the encoding scheme\n",
    "def transform_sequence(sequence, encoding):\n",
    "    numerical_data = []\n",
    "    for i in range(0, len(sequence), 2):  # Assuming the sequences can be divided into di-nucleotides\n",
    "        di_nucleotide = sequence[i:i+2]\n",
    "        numerical_data.append(encoding.get(di_nucleotide, 0))  # Default 0 for unknown di-nucleotides\n",
    "    \n",
    "    # Split the numerical data into non-overlapping windows of size 10 and calculate the average\n",
    "    window_avgs = [np.mean(numerical_data[i:i+10]) for i in range(0, len(numerical_data), 10)]\n",
    "    return window_avgs\n",
    "\n",
    "# Encoding scheme\n",
    "encoding = {\n",
    "    'AA': 0.00, 'AC': 0.50, 'AG': 0.50, 'AT': 0.00,\n",
    "    'CA': 0.50, 'CC': 1.00, 'CG': 1.00, 'CT': 0.50,\n",
    "    'GA': 0.50, 'GC': 1.00, 'GG': 1.00, 'GT': 0.50,\n",
    "    'TA': 0.00, 'TC': 0.50, 'TG': 0.50, 'TT': 0.00\n",
    "}\n",
    "\n",
    "# Function to load and transform data\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    # Handle varying window counts by padding with NaN and later imputing or dropping\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "# Load and transform data\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    # Excluded LDA and QDA due to collinearity warning\n",
    "    'SVM': SVC(probability=True, random_state=101),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=101),\n",
    "    'Logistic Regression': LogisticRegression(random_state=101),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=101),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=101),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=101),\n",
    "    #'Perceptron': Perceptron(random_state=101),\n",
    "    #'SGD': SGDClassifier(random_state=101),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=101),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=101),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=101),\n",
    "    'LightGBM': LGBMClassifier(random_state=101),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)\n",
    "}\n",
    "\n",
    "\n",
    "# Train individual classifiers and print their performance\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"    Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"    Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
    "    print(f\"    F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\\n\")\n",
    "\n",
    "# Create a VotingClassifier with soft voting (ensure all classifiers support predict_proba)\n",
    "voting_clf_soft = VotingClassifier(estimators=list(classifiers.items()), voting='soft')\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "print(\"Soft Voting Classifier Performance:\")\n",
    "print(f\"    Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "print(f\"    Precision: {precision_score(y_test, y_pred_soft, zero_division=0):.4f}\")\n",
    "print(f\"    Recall: {recall_score(y_test, y_pred_soft, zero_division=0):.4f}\")\n",
    "print(f\"    F1 Score: {f1_score(y_test, y_pred_soft, zero_division=0):.4f}\")\n",
    "\n",
    "# Optionally, create and evaluate a VotingClassifier with hard voting\n",
    "voting_clf_hard = VotingClassifier(estimators=list(classifiers.items()), voting='hard')\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "y_pred_hard = voting_clf_hard.predict(X_test)\n",
    "print(\"Hard Voting Classifier Performance:\")\n",
    "print(f\"    Accuracy: {accuracy_score(y_test, y_pred_hard):.4f}\")\n",
    "print(f\"    Precision: {precision_score(y_test, y_pred_hard, zero_division=0):.4f}\")\n",
    "print(f\"    Recall: {recall_score(y_test, y_pred_hard, zero_division=0):.4f}\")\n",
    "print(f\"    F1 Score: {f1_score(y_test, y_pred_hard, zero_division=0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9e2e62-f752-4e53-9704-ef95011daada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 264, number of negative: 283\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 357\n",
      "[LightGBM] [Info] Number of data points in the train set: 547, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482633 -> initscore=-0.069498\n",
      "[LightGBM] [Info] Start training from score -0.069498\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 211, number of negative: 226\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 348\n",
      "[LightGBM] [Info] Number of data points in the train set: 437, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482838 -> initscore=-0.068677\n",
      "[LightGBM] [Info] Start training from score -0.068677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 211, number of negative: 226\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 437, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.482838 -> initscore=-0.068677\n",
      "[LightGBM] [Info] Start training from score -0.068677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 211, number of negative: 227\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 438, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.481735 -> initscore=-0.073092\n",
      "[LightGBM] [Info] Start training from score -0.073092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 211, number of negative: 227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 342\n",
      "[LightGBM] [Info] Number of data points in the train set: 438, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.481735 -> initscore=-0.073092\n",
      "[LightGBM] [Info] Start training from score -0.073092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 212, number of negative: 226\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 353\n",
      "[LightGBM] [Info] Number of data points in the train set: 438, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.484018 -> initscore=-0.063949\n",
      "[LightGBM] [Info] Start training from score -0.063949\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Stacking Classifier Performance:\n",
      "Accuracy: 0.8029\n",
      "Precision: 0.8400\n",
      "Recall: 0.8077\n",
      "F1 Score: 0.8235\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Function to transform sequence data\n",
    "def transform_sequence(sequence, encoding):\n",
    "    numerical_data = []\n",
    "    for i in range(0, len(sequence), 2):\n",
    "        di_nucleotide = sequence[i:i+2]\n",
    "        numerical_data.append(encoding.get(di_nucleotide, 0))\n",
    "    window_avgs = [np.mean(numerical_data[i:i+10]) for i in range(0, len(numerical_data), 10)]\n",
    "    return window_avgs\n",
    "\n",
    "# Encoding scheme\n",
    "encoding = {\n",
    "    'AA': 0.00, 'AC': 0.50, 'AG': 0.50, 'AT': 0.00,\n",
    "    'CA': 0.50, 'CC': 1.00, 'CG': 1.00, 'CT': 0.50,\n",
    "    'GA': 0.50, 'GC': 1.00, 'GG': 1.00, 'GT': 0.50,\n",
    "    'TA': 0.00, 'TC': 0.50, 'TG': 0.50, 'TT': 0.00\n",
    "}\n",
    "\n",
    "# Load and transform data function\n",
    "def load_and_transform_data(pos_file_path, neg_file_path, encoding):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, label in [(pos_file_path, 1), (neg_file_path, 0)]:\n",
    "        with open(file_path, 'r') as file:\n",
    "            sequences = file.read().split('\\n')\n",
    "        transformed_data = [transform_sequence(seq, encoding) for seq in sequences if seq]\n",
    "        labels = [label] * len(transformed_data)\n",
    "        df = pd.DataFrame(transformed_data)\n",
    "        df['label'] = labels\n",
    "        combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
    "    combined_data = combined_data.apply(lambda x: pd.Series(x.dropna().values)).fillna(0)\n",
    "    return combined_data.drop('label', axis=1), combined_data['label']\n",
    "\n",
    "# Load and transform data\n",
    "X, y = load_and_transform_data('oripos.txt', 'orineg.txt', encoding)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# Base classifiers for stacking\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=101)),\n",
    "    ('svc', SVC(probability=True, random_state=101)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=101)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=101)),\n",
    "    ('lr', LogisticRegression(random_state=101)),\n",
    "    ('gnb', GaussianNB()),\n",
    "    ('adb', AdaBoostClassifier(n_estimators=100, random_state=101)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=100, random_state=101)),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)),\n",
    "    ('lgbm', LGBMClassifier(random_state=101)),\n",
    "    ('catboost', CatBoostClassifier(verbose=0, random_state=101)),\n",
    "    ('cal_sgd', CalibratedClassifierCV(SGDClassifier(random_state=101), method='sigmoid', cv=5))\n",
    "]\n",
    "\n",
    "# Meta-classifier for stacking\n",
    "meta_classifier = RandomForestClassifier(n_estimators=100, random_state=101)\n",
    "\n",
    "# Stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier, cv=5)\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "print(\"Stacking Classifier Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_stack, zero_division=0):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_stack, zero_division=0):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred_stack, zero_division=0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c39be8-7b9b-4514-b077-eb01c4e29b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 62.75 seconds\n",
      "                             Combination           Classifier  Accuracy  \\\n",
      "0   Pos: (5000, 5100), Neg: (1000, 1100)        Random Forest  0.485342   \n",
      "1   Pos: (5000, 5100), Neg: (1000, 1100)                  SVM  0.488599   \n",
      "2   Pos: (5000, 5100), Neg: (1000, 1100)  Logistic Regression  0.537459   \n",
      "3   Pos: (5000, 5100), Neg: (1000, 1100)          Naive Bayes  0.534202   \n",
      "4   Pos: (5000, 5100), Neg: (1000, 1100)                 K-NN  0.547231   \n",
      "..                                   ...                  ...       ...   \n",
      "65  Pos: (5000, 5500), Neg: (1000, 1500)                  SGD  0.765472   \n",
      "66  Pos: (5000, 5500), Neg: (1000, 1500)              Bagging  0.713355   \n",
      "67  Pos: (5000, 5500), Neg: (1000, 1500)          Extra Trees  0.716612   \n",
      "68  Pos: (5000, 5500), Neg: (1000, 1500)             CatBoost  0.752443   \n",
      "69  Pos: (5000, 5500), Neg: (1000, 1500)              XGBoost  0.732899   \n",
      "\n",
      "    Precision    Recall  F1 Score  \n",
      "0    0.474026  0.486667  0.480263  \n",
      "1    0.488599  1.000000  0.656455  \n",
      "2    0.527778  0.506667  0.517007  \n",
      "3    0.520000  0.606667  0.560000  \n",
      "4    0.534161  0.573333  0.553055  \n",
      "..        ...       ...       ...  \n",
      "65   0.740741  0.800000  0.769231  \n",
      "66   0.693750  0.740000  0.716129  \n",
      "67   0.700637  0.733333  0.716612  \n",
      "68   0.710227  0.833333  0.766871  \n",
      "69   0.702381  0.786667  0.742138  \n",
      "\n",
      "[70 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Encoding schemes\n",
    "encodings = {\n",
    "    'free_energy': {\n",
    "        'AA': -1.0, 'AC': -1.44, 'AG': -1.28, 'AT': -0.88, \n",
    "        'CA': -1.45, 'CC': -1.84, 'CG': -2.17, 'CT': -1.28, \n",
    "        'GA': -1.30, 'GC': -2.24, 'GG': -1.84, 'GT': -1.44, \n",
    "        'TA': -0.58, 'TC': -1.30, 'TG': -1.45, 'TT': -1.0\n",
    "    },\n",
    "    # Add other encodings here if needed\n",
    "}\n",
    "\n",
    "# Function to calculate dinucleotide perplexity\n",
    "def calculate_perplexity(seq, window_size=17):\n",
    "    if len(seq) < window_size:\n",
    "        return []\n",
    "\n",
    "    seq_array = np.array(list(seq))\n",
    "    perplexities = []\n",
    "\n",
    "    for i in range(0, len(seq) - window_size + 1, 8):\n",
    "        window = seq_array[i:i + window_size]\n",
    "        dinucleotide_counts = Counter(\"\".join(window[j:j + 2]) for j in range(window_size - 1))\n",
    "        total_dinucleotides = window_size - 1\n",
    "\n",
    "        probabilities = {k: v / total_dinucleotides for k, v in dinucleotide_counts.items()}\n",
    "        entropy = -sum(p * math.log2(p) for p in probabilities.values() if p > 0)\n",
    "        perplexity = 2 ** entropy\n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "    return perplexities\n",
    "\n",
    "# Function to transform sequence data\n",
    "def transform_sequence(sequence, encodings, window_size=17):\n",
    "    transformed = []\n",
    "    for encoding in encodings.values():\n",
    "        value = sum(encoding.get(sequence[i:i+2], 0) for i in range(len(sequence) - 1)) / (len(sequence) - 1)\n",
    "        transformed.append(value)\n",
    "    \n",
    "    # Add perplexity as an additional feature\n",
    "    perplexities = calculate_perplexity(sequence, window_size)\n",
    "    if perplexities:\n",
    "        transformed.append(np.mean(perplexities))\n",
    "    else:\n",
    "        transformed.append(0)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "# Function to load and transform data from specific regions\n",
    "def load_and_transform_data(file_path, pos_range, neg_range, encodings):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sequences = file.read().split('\\n')\n",
    "    \n",
    "    pos_sequences = [seq[pos_range[0]-1:pos_range[1]] for seq in sequences if len(seq) >= pos_range[1]]\n",
    "    neg_sequences = [seq[neg_range[0]-1:neg_range[1]] for seq in sequences if len(seq) >= neg_range[1]]\n",
    "    \n",
    "    pos_data = pd.DataFrame([transform_sequence(seq, encodings) for seq in pos_sequences])\n",
    "    pos_data['label'] = 1\n",
    "    neg_data = pd.DataFrame([transform_sequence(seq, encodings) for seq in neg_sequences])\n",
    "    neg_data['label'] = 0\n",
    "    \n",
    "    return pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=101),\n",
    "    'SVM': SVC(probability=True, random_state=101),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=101),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=101),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=101),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=101),\n",
    "    'Perceptron': SGDClassifier(loss='perceptron', random_state=101),\n",
    "    'SGD': SGDClassifier(random_state=101),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=101),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=101),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=101, iterations=100),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)\n",
    "}\n",
    "\n",
    "# Combinations of positive and negative sequence ranges\n",
    "combinations = [\n",
    "    ((5000, 5100), (1000, 1100)),\n",
    "    ((5000, 5200), (1000, 1200)),\n",
    "    ((5000, 5300), (1000, 1300)),\n",
    "    ((5000, 5400), (1000, 1400)),\n",
    "    ((5000, 5500), (1000, 1500))\n",
    "]\n",
    "\n",
    "# Main process\n",
    "start_time = time.time()\n",
    "results = []\n",
    "for pos_range, neg_range in combinations:\n",
    "    data = load_and_transform_data('Arabidopsis_correct_10001', pos_range, neg_range, encodings)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop('label', axis=1), data['label'], test_size=0.1, random_state=101)\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results.append({\n",
    "            'Combination': f'Pos: {pos_range}, Neg: {neg_range}',\n",
    "            'Classifier': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Execution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df.to_excel('classification_results.xlsx', index=False)\n",
    "\n",
    "# Save to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fac3375-06d2-4cc6-a602-33c647ef733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 48.03 seconds\n",
      "                             Combination           Classifier  Accuracy  \\\n",
      "0   Pos: (5000, 5100), Neg: (1000, 1100)        Random Forest  0.514658   \n",
      "1   Pos: (5000, 5100), Neg: (1000, 1100)                  SVM  0.501629   \n",
      "2   Pos: (5000, 5100), Neg: (1000, 1100)  Logistic Regression  0.498371   \n",
      "3   Pos: (5000, 5100), Neg: (1000, 1100)          Naive Bayes  0.521173   \n",
      "4   Pos: (5000, 5100), Neg: (1000, 1100)                 K-NN  0.462541   \n",
      "..                                   ...                  ...       ...   \n",
      "65  Pos: (5000, 5500), Neg: (1000, 1500)                  SGD  0.638436   \n",
      "66  Pos: (5000, 5500), Neg: (1000, 1500)              Bagging  0.527687   \n",
      "67  Pos: (5000, 5500), Neg: (1000, 1500)          Extra Trees  0.517915   \n",
      "68  Pos: (5000, 5500), Neg: (1000, 1500)             CatBoost  0.622150   \n",
      "69  Pos: (5000, 5500), Neg: (1000, 1500)              XGBoost  0.557003   \n",
      "\n",
      "    Precision    Recall  F1 Score  \n",
      "0    0.503311  0.506667  0.504983  \n",
      "1    0.490909  0.540000  0.514286  \n",
      "2    0.491870  0.806667  0.611111  \n",
      "3    0.506438  0.786667  0.616188  \n",
      "4    0.452830  0.480000  0.466019  \n",
      "..        ...       ...       ...  \n",
      "65   0.607735  0.733333  0.664653  \n",
      "66   0.517241  0.500000  0.508475  \n",
      "67   0.506849  0.493333  0.500000  \n",
      "68   0.589474  0.746667  0.658824  \n",
      "69   0.539326  0.640000  0.585366  \n",
      "\n",
      "[70 rows x 6 columns]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'classification_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_results.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Save to a CSV file\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3903\u001b[0m     path_or_buf,\n\u001b[0;32m   3904\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3905\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3906\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3907\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3908\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3909\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3910\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3911\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3912\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3913\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3914\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3915\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3916\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3917\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3918\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3919\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    251\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    252\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    253\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    254\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'classification_results.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Function to calculate dinucleotide perplexity\n",
    "def calculate_perplexity(seq, window_size=17):\n",
    "    if len(seq) < window_size:\n",
    "        return []\n",
    "\n",
    "    seq_array = np.array(list(seq))\n",
    "    perplexities = []\n",
    "\n",
    "    for i in range(0, len(seq) - window_size + 1, 8):\n",
    "        window = seq_array[i:i + window_size]\n",
    "        dinucleotide_counts = Counter(\"\".join(window[j:j + 2]) for j in range(window_size - 1))\n",
    "        total_dinucleotides = window_size - 1\n",
    "\n",
    "        probabilities = {k: v / total_dinucleotides for k, v in dinucleotide_counts.items()}\n",
    "        entropy = -sum(p * math.log2(p) for p in probabilities.values() if p > 0)\n",
    "        perplexity = 2 ** entropy\n",
    "        perplexities.append(perplexity)\n",
    "    \n",
    "    return perplexities\n",
    "\n",
    "# Function to transform sequence data\n",
    "def transform_sequence(sequence, window_size=17):\n",
    "    transformed = []\n",
    "\n",
    "    # Add perplexity as the only feature\n",
    "    perplexities = calculate_perplexity(sequence, window_size)\n",
    "    if perplexities:\n",
    "        transformed.append(np.mean(perplexities))\n",
    "    else:\n",
    "        transformed.append(0)\n",
    "    \n",
    "    return transformed\n",
    "\n",
    "# Function to load and transform data from specific regions\n",
    "def load_and_transform_data(file_path, pos_range, neg_range):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sequences = file.read().split('\\n')\n",
    "    \n",
    "    pos_sequences = [seq[pos_range[0]-1:pos_range[1]] for seq in sequences if len(seq) >= pos_range[1]]\n",
    "    neg_sequences = [seq[neg_range[0]-1:neg_range[1]] for seq in sequences if len(seq) >= neg_range[1]]\n",
    "    \n",
    "    pos_data = pd.DataFrame([transform_sequence(seq) for seq in pos_sequences])\n",
    "    pos_data['label'] = 1\n",
    "    neg_data = pd.DataFrame([transform_sequence(seq) for seq in neg_sequences])\n",
    "    neg_data['label'] = 0\n",
    "    \n",
    "    return pd.concat([pos_data, neg_data], ignore_index=True)\n",
    "\n",
    "# Classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=101),\n",
    "    'SVM': SVC(probability=True, random_state=101),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=101),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-NN': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=101),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=101),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=101),\n",
    "    'Perceptron': SGDClassifier(loss='perceptron', random_state=101),\n",
    "    'SGD': SGDClassifier(random_state=101),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=101),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=101),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=101, iterations=100),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=101)\n",
    "}\n",
    "\n",
    "# Combinations of positive and negative sequence ranges\n",
    "combinations = [\n",
    "    ((5000, 5100), (1000, 1100)),\n",
    "    ((5000, 5200), (1000, 1200)),\n",
    "    ((5000, 5300), (1000, 1300)),\n",
    "    ((5000, 5400), (1000, 1400)),\n",
    "    ((5000, 5500), (1000, 1500))\n",
    "]\n",
    "\n",
    "# Main process\n",
    "start_time = time.time()\n",
    "results = []\n",
    "for pos_range, neg_range in combinations:\n",
    "    data = load_and_transform_data('Arabidopsis_correct_10001', pos_range, neg_range)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop('label', axis=1), data['label'], test_size=0.1, random_state=101)\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        results.append({\n",
    "            'Combination': f'Pos: {pos_range}, Neg: {neg_range}',\n",
    "            'Classifier': name,\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"Execution Time: {time.time() - start_time:.2f} seconds\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to an Excel file\n",
    "results_df.to_excel('classification_results.xlsx', index=False)\n",
    "\n",
    "# Save to a CSV file\n",
    "results_df.to_csv('classification_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473cb7c6-eaa9-4d5c-b677-f5816dff0454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
